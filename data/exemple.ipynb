{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def detect_historical_facts_v3(text, city_name=\"\", only_street_facts=False):\n",
    "\n",
    "    historical_facts = []\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    date_pattern = r\"\\b(?:\\d{1,2}\\s)?(?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre|\\d{1,2}(?:er)?\\s(?:siècle)|X{1,3}V{0,1}I{0,3}e\\s(?:siècle)|\\d{4})\\b\"\n",
    "    date_matches = [match for match in re.finditer(\n",
    "        date_pattern, text, re.IGNORECASE)]\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    date_token_ids = []\n",
    "\n",
    "    for match in date_matches:\n",
    "        date_start, date_end = match.span()\n",
    "        date_text = match.group()\n",
    "\n",
    "        char_span = doc.char_span(date_start, date_end)\n",
    "        if char_span is not None:\n",
    "            token_start = char_span.start\n",
    "            date_token_ids.append(token_start)\n",
    "\n",
    "    pattern = [{\"POS\": \"NUM\", \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": date_pattern}}, {\n",
    "        \"POS\": \"ADP\", \"OP\": \"?\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "    matcher.add(\"DATE_PATTERN\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    coords = {}\n",
    "    for match_id, start, end in matches:\n",
    "        if start in date_token_ids:\n",
    "            sent = doc[start:end].sent\n",
    "            if only_street_facts:\n",
    "                for street_name in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\", \"église\", \"île\", \"chapelle\"]:\n",
    "                    if street_name in sent.text.lower():\n",
    "                        fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\": [], \"entities_org\": [], \"description\": sent.text}\n",
    "                        for ent in sent.ents:\n",
    "                            if ent.label_ == \"LOC\":\n",
    "                                fact[\"entities_loc\"].append(ent.text)\n",
    "                            elif ent.label_ == \"PER\":\n",
    "                                fact[\"entities_per\"].append(ent.text)\n",
    "                            elif ent.label_ == \"ORG\":\n",
    "                                fact[\"entities_org\"].append(ent.text)\n",
    "                                '''\n",
    "                                if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                                    if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                                        coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                                    fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                                else:\n",
    "                                    fact[\"coordinates\"] = (None, None)\n",
    "                                '''\n",
    "\n",
    "                        historical_facts.append(fact)\n",
    "                        break\n",
    "            else:\n",
    "\n",
    "                fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\": [], \"entities_org\": [], \"description\": sent.text}\n",
    "                for ent in sent.ents:\n",
    "                    if ent.label_ == \"LOC\":\n",
    "                        fact[\"entities_loc\"].append(ent.text)\n",
    "                    elif ent.label_ == \"PER\":\n",
    "                        fact[\"entities_per\"].append(ent.text)\n",
    "                    elif ent.label_ == \"ORG\":\n",
    "                        fact[\"entities_org\"].append(ent.text)\n",
    "                '''\n",
    "                if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                    if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                        coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                    fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                else:\n",
    "                    fact[\"coordinates\"] = (None, None)\n",
    "                '''\n",
    "                historical_facts.append(fact)\n",
    "    #historical_facts = [fact for fact in historical_facts if np.array([street in fact[\"description\"].lower() for street in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\",\"église\",\"île\",\"chapelle\"]]).any()]\n",
    "    return historical_facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.lagarennecolombes.fr/Histoire%20et%20patrimoine/6042/6164\"\n",
    "def extract_historic_facts_regex(text):\n",
    "    # Expression régulière pour identifier les dates et les événements historiques\n",
    "    # Expression régulière pour identifier les dates\n",
    "    date_pattern = r\"(\\b(?:\\d{1,4}|X{1,3}I{1,3}|X{1,3}V?I{0,3}|V?I{1,3})\\s?(?:siècle|année|époque)\\b)\"\n",
    "\n",
    "    # Expression régulière pour identifier les lieux (rues, places, avenues, boulevards)\n",
    "    location_pattern = r\"(\\b(?:rue|place|avenue|boulevard)[^\\n,.!?]*\\b)\"\n",
    "\n",
    "    # Combinaison des expressions régulières\n",
    "    regex = re.compile(f\"({date_pattern}|{location_pattern})\", re.IGNORECASE)\n",
    "\n",
    "    # Découper le texte en phrases en utilisant un point, un point d'interrogation ou un point d'exclamation comme séparateur\n",
    "    sentences = re.split(r\"[.!?]\", text)\n",
    "    # Trouver les phrases qui correspondent à l'expression régulière\n",
    "    historic_facts = [sentence.strip() for sentence in sentences if regex.search(sentence)]\n",
    "\n",
    "    return historic_facts\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content_section = soup.find(\"div\", {\"class\": \"detail\"})\n",
    "    if content_section:\n",
    "        text = content_section.get_text()\n",
    "historic_facts = extract_historic_facts_regex(text)\n",
    "\n",
    "for fact in historic_facts:\n",
    "    print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cities = json.load(open(\"citiesv3.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "count = 0\n",
    "\n",
    "for city in cities:\n",
    "    if count < 10:\n",
    "        if city[\"name\"] != \"Marseille\":\n",
    "            query = f\"histoire de {city['name']}\"\n",
    "            print(query)\n",
    "            for j in search(query, num_results=1):\n",
    "                print(j)\n",
    "            \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_section_contents(url, regex_list):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    sections = \"\"\n",
    "\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex, re.IGNORECASE)\n",
    "\n",
    "        headers = soup.find_all(lambda tag: tag.name in {\"h2\"} and pattern.match(tag.text))\n",
    "\n",
    "        for header in headers:\n",
    "            content = \"\"\n",
    "            for sibling in header.find_next_siblings():\n",
    "                if sibling.name in {\"h2\"}:\n",
    "                    break\n",
    "                if sibling.name in {\"h3\", \"h4\", \"h5\", \"h6\"}:\n",
    "                    continue\n",
    "                sibling_text = re.sub(r'\\[\\d+\\]', '', sibling.text)\n",
    "                content += sibling_text.strip() + \"\\n\"\n",
    "\n",
    "            #section_title = header.text.strip().split('[')[0]\n",
    "            sections += content\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://fr.wikipedia.org/wiki/Auvergne-Rh%C3%B4ne-Alpes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "count = 0\n",
    "facts = {}\n",
    "last_len = 0\n",
    "for city in cities[:10]:\n",
    "    sections = get_section_contents(city[\"wikipediaUrl\"], regex_list)\n",
    "    facts[city[\"id\"]] = detect_historical_facts_v3(sections, only_street_facts=False)\n",
    "    print(city[\"name\"], len(facts[city[\"id\"]]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates\n",
    "import requests\n",
    "def get_coordinates(address):\n",
    "    try:\n",
    "        params = {\n",
    "        \"q\": address,\n",
    "        \"format\": \"jsonv2\"\n",
    "        }\n",
    "        header = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(\"https://nominatim.openstreetmap.org/search\", params=params, headers=header)\n",
    "        result = response.json()[0]\n",
    "\n",
    "        latitude = result[\"lat\"]\n",
    "        longitude = result[\"lon\"]\n",
    "    \n",
    "    except:\n",
    "        latitude = None\n",
    "        longitude = None\n",
    "    \n",
    "    return latitude, longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('48.935773', '2.3580232')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates(\"Saint-Denis Saint-Denis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal string\n",
    "from fuzzywuzzy import fuzz\n",
    "def equal_string(s1, s2, threshold=90):\n",
    "    return fuzz.token_set_ratio(s1, s2) > 90\n",
    "\n",
    "s1 = \"boulevard de verdun\"\n",
    "s2 = \"avenue de verdun\"\n",
    "equal_string(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_facts_wikipedia(url, city_name=\"\", only_streets=False):\n",
    "    regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "    histoire = get_section_contents(url, regex_list)\n",
    "    historic_facts = detect_historical_facts_v3(histoire, only_street_facts=only_streets)\n",
    "    return historic_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "facts = {}\n",
    "last_len = 0\n",
    "for city in cities[:10]:\n",
    "    facts[city[\"id\"]] = find_facts_wikipedia(city[\"wikipediaUrl\"], city[\"name\"], False)\n",
    "    #facts[city[\"id\"]] = [fact for fact in facts[city[\"id\"]] if np.array([street in fact[\"description\"].lower() for street in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\",\"église\",\"île\",\"chapelle\"]]).any()]\n",
    "    print(city[\"name\"], len(facts[city[\"id\"]]))\n",
    "    count += 1\n",
    "    if count >0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('street_facts_v2.json','r', encoding='utf-8') as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetNames = [\" rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"impasse\", \"square\", \"chemin\", \"rond-point\", \"pont\",  \"esplanade\", \"promenade\", \"voie\", \"cathédrale\", \"église\", \"île\", \"chapelle\", \"collège\", \"lycée\",\"mairie\", \"école\", \"cimetière\", \"hôpital\", \"musée\", \"théâtre\", \"cinéma\", \"gare\", \"piscine\", \"parc\", \"jardin\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = {\n",
    "    \"rue\" : 0.0005,\n",
    "    \"avenue\" : 0.001,\n",
    "    \"boulevard\" : 0.002,\n",
    "    \"place\" : 0.0008,\n",
    "    \"allée\" : 0.0002,\n",
    "    \"impasse\" : 0.0001,\n",
    "    \"chemin\" : 0.0003,\n",
    "    \"cours\" : 0.0004,\n",
    "    \"quai\" : 0.0006,\n",
    "    \"passage\" : 0.0007,\n",
    "    \"square\" : 0.0009,\n",
    "    \"route\" : 0.0011,\n",
    "    \"rond-point\" : 0.001,\n",
    "    \"voie\" : 0.0005,\n",
    "    \"promenade\" : 0.0002,\n",
    "    \"parc\" : 0.001,\n",
    "    \"cité\" : 0.0008,\n",
    "    \"esplanade\" : 0.001,\n",
    "    \"cathédrale\" : 0.0001,\n",
    "    \"église\" : 0.0001,\n",
    "    \"île\" : 0.001,\n",
    "    \"chapelle\" : 0.0001,\n",
    "    \"collège\" : 0.00001,\n",
    "    \"lycée\" : 0.00001,\n",
    "    \"mairie\" : 0.00001,\n",
    "    \"école\" : 0.00001,\n",
    "    \"cimetière\" : 0.00001,\n",
    "    \"hôpital\" : 0.00001,\n",
    "    \"musée\" : 0.00001,\n",
    "    \"théâtre\" : 0.00001,\n",
    "    \"cinéma\" : 0.00001,\n",
    "    \"gare\" : 0.00001,\n",
    "    \"piscine\" : 0.00001,\n",
    "    \"jardin\" : 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('street_facts_v3.json','w', encoding='utf-8') as f:\n",
    "    json.dump(facts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"citiesv3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "known_places = {}\n",
    "street_fact = {}\n",
    "i = 0\n",
    "for city in facts.keys():\n",
    "    street_fact[city] = []\n",
    "    for fact in facts[city]:\n",
    "        for street in streetNames:\n",
    "            if street+\" \" in fact[\"description\"].lower() or street+\".\" in fact[\"description\"].lower():\n",
    "                fact[\"area\"] = size[street] if street in size.keys() else None\n",
    "                if len(fact[\"entities_loc\"]) != 0:\n",
    "                    key_place = max(fact[\"entities_loc\"])+ \" \" + cities[city][\"name\"]\n",
    "                    if key_place not in known_places.keys():\n",
    "                        known_places[key_place] = get_coordinates(key_place)\n",
    "                    fact[\"coord\"] = known_places[key_place]\n",
    "                    fact[\"street_loc\"] = True\n",
    "                    if fact[\"coord\"] == (None, None):\n",
    "                        fact[\"coord\"] = cities[city][\"coordinates\"]\n",
    "                        fact[\"street_loc\"] = False\n",
    "                else:\n",
    "                    fact[\"coord\"] = cities[city][\"coordinates\"]\n",
    "                    fact[\"street_loc\"] = False\n",
    "                print(fact[\"coord\"], key_place)\n",
    "                street_fact[city].append(fact)\n",
    "                break\n",
    "    i += 1\n",
    "    print(i, end=\"-\")\n",
    "    if i > 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('street_facts_v3.json','w', encoding='utf-8') as f:\n",
    "    json.dump(facts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(\"./facts_dep_reg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./regions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    regions = json.load(f)\n",
    "with open(\"./departements.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    departements = json.load(f)\n",
    "\n",
    "dep_regs = {**regions, **departements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_places = {}\n",
    "street_fact = {}\n",
    "non_street_fact = {}\n",
    "i = 0\n",
    "for city in facts.keys():\n",
    "    street_fact[city] = []\n",
    "    non_street_fact[city] = []\n",
    "    for fact in facts[city]:\n",
    "        found_street = False\n",
    "        for street in streetNames:\n",
    "            if any(street+ delim in fact[\"description\"].lower() for delim in [\" \",\".\"]):\n",
    "                if(len(fact[\"entities_loc\"]) != 0):\n",
    "                    key_place = max(fact[\"entities_loc\"])+ \" \" + dep_regs[city][\"name\"]\n",
    "                    if key_place not in known_places.keys():\n",
    "                        known_places[key_place] = get_coordinates(key_place)\n",
    "                    fact[\"coordinates\"] = known_places[key_place]\n",
    "                    fact[\"street_loc\"] = True\n",
    "                    if fact[\"coordinates\"] == (None, None):\n",
    "                        fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "                        fact[\"street_loc\"] = False\n",
    "                else:\n",
    "                    fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "                    fact[\"street_loc\"] = False\n",
    "                fact[\"area\"] = size[street] if street in size.keys() else None\n",
    "                street_fact[city].append(fact)\n",
    "                found_street = True\n",
    "                break\n",
    "        if not found_street:\n",
    "            fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "            fact[\"area\"] = dep_regs[city][\"area\"]\n",
    "            non_street_fact[city].append(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73774\n",
      "353627\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for streetfact in street_fact:\n",
    "    count += len(street_fact[streetfact])\n",
    "print(count)\n",
    "\n",
    "for nonstreetfact in non_street_fact:\n",
    "    count += len(non_street_fact[nonstreetfact])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353627\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for factnez in facts:\n",
    "    count += len(facts[factnez])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"street_facts_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(street_fact, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_street_facts_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(non_street_fact, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "facts = json.load(open(\"./facts_dep_reg.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_duplicates = {}\n",
    "count = 0\n",
    "for city in facts.keys():\n",
    "    no_duplicates[city] = []\n",
    "    last_fact = \"\"\n",
    "    for fact in facts[city]:\n",
    "        if fact[\"description\"] in last_fact:            \n",
    "            no_duplicates[city][-1][\"date\"] += \"\" if any([date in no_duplicates[city][-1][\"date\"] for date in fact[\"date\"].split(\" \")]) else \" \" + fact[\"date\"]\n",
    "            no_duplicates[city][-1][\"entities_loc\"] += [fact_ for fact_ in fact[\"entities_loc\"] if fact_ not in no_duplicates[city][-1][\"entities_loc\"]]\n",
    "            no_duplicates[city][-1][\"entities_per\"] += [fact_ for fact_ in fact[\"entities_per\"] if fact_ not in no_duplicates[city][-1][\"entities_per\"]]\n",
    "            no_duplicates[city][-1][\"entities_org\"] += [fact_ for fact_ in fact[\"entities_org\"] if fact_ not in no_duplicates[city][-1][\"entities_org\"]]\n",
    "        else:\n",
    "            no_duplicates[city].append(fact)\n",
    "            count += 1\n",
    "        last_fact = fact[\"description\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./street_facts_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./regions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    regions = json.load(f)\n",
    "with open(\"./departements.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    departements = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "clean_facts = {}\n",
    "vague_terms = [\"cela\", \"ceci\", \"ça\",\"il\",\"elle\",\"ils\",\"elles\",\"on\",\"se\",\"sa\",\"son\",\"ses\",\"leur\",\"leurs\"]\n",
    "for city, facts_list in facts.items():\n",
    "    clean_facts[city] = []\n",
    "    for fact in facts_list:\n",
    "        add_fact = True\n",
    "        fact[\"description\"] = fact[\"description\"].strip(\" \\n\")\n",
    "        if fact[\"description\"].split(\" \")[0].lower() in vague_terms or fact[\"description\"][-1] not in [\".\",\"!\",\"?\",\";\"] or fact[\"description\"][0].islower():\n",
    "            add_fact = False\n",
    "        if add_fact:       \n",
    "            clean_facts[city].append(fact)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58331\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for city, facts_list in clean_facts.items():\n",
    "    count += len(facts_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "DATABASE_URL=\"mongodb+srv://antoninmarxer:5f3aac58ed65413a8072d625492ec401011eecbaafbc3e27a749e3dcf7ef4560@cluster0.a0nfhg5.mongodb.net/test?retryWrites=true&w=majorit\"\n",
    "\n",
    "client = MongoClient(DATABASE_URL)\n",
    "\n",
    "# Remplacez 'your_database_name' par le nom de votre base de données\n",
    "db = client.test\n",
    "\n",
    "# Remplacez 'your_collection_name' par le nom de la collection que vous souhaitez utiliser\n",
    "collection = db.facts\n",
    "\n",
    "# Inserez many\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"facts_city_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '1738', 'entities_loc': ['de Saint-Denis'], 'entities_per': ['Bertrand-François Mahé de La Bourdonnais'], 'entities_org': [], 'description': \"Cette dualité entraînera un partage de pouvoir et d'influence entre les deux cités, qui basculera au profit de Saint-Denis en 1738, lorsque le gouverneur Bertrand-François Mahé de La Bourdonnais lui conféra le statut de chef-lieu.\", 'coordinates': 'Point(55.448055555 -20.878888888)', 'area': '142.79', 'type': 'city'}\n"
     ]
    }
   ],
   "source": [
    "print(facts[list(facts.keys())[0]][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
