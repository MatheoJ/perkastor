{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def fetch_historic_facts_v2(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Récupérer et afficher le titre de la page\n",
    "        title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text\n",
    "        print(f\"Titre de la page : {title}\")\n",
    "\n",
    "        # Trouver la rubrique \"Histoire\"\n",
    "        histoire_header = soup.find(\"span\", {\"id\": \"Histoire\"})\n",
    "        if histoire_header:\n",
    "            histoire_section = histoire_header.find_parent(\"h2\")\n",
    "            content = histoire_section.find_next_sibling()\n",
    "\n",
    "        # Afficher le contenu texte de chaque paragraphe de la rubrique \"Histoire\"\n",
    "        text = \"\"\n",
    "        while content and content.name != \"h2\":\n",
    "            if content.name == \"p\":\n",
    "                text += content.text.strip() + \" \"\n",
    "            content = content.find_next_sibling()\n",
    "        #print(text)\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        historic_facts = []\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            if ent.label_ in [\"DATE\", \"EVENT\"] or np.array([street in ent.text for street in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\"]]).sum() > 0:\n",
    "                sentence = ent.sent.text.strip()\n",
    "                if sentence not in historic_facts:\n",
    "                    historic_facts.append(sentence)\n",
    "\n",
    "            return historic_facts\n",
    "        else:\n",
    "            print(\"La section de contenu n'a pas été trouvée sur la page.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"Erreur lors de la récupération de la page. Code d'erreur : {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "url = \"https://fr.wikipedia.org/wiki/La_Garenne-Colombes\"\n",
    "historic_facts = fetch_historic_facts_v2(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def detect_historical_facts_v3(text, city_name=\"\", only_street_facts=False):\n",
    "\n",
    "    historical_facts = []\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    date_pattern = r\"\\b(?:\\d{1,2}\\s)?(?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre|\\d{1,2}(?:er)?\\s(?:siècle)|X{1,3}V{0,1}I{0,3}e\\s(?:siècle)|\\d{4})\\b\"\n",
    "    date_matches = [match for match in re.finditer(date_pattern, text, re.IGNORECASE)]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    date_token_ids = []\n",
    "\n",
    "    for match in date_matches:\n",
    "        date_start, date_end = match.span()\n",
    "        date_text = match.group()\n",
    "        \n",
    "        char_span = doc.char_span(date_start, date_end)\n",
    "        if char_span is not None:\n",
    "            token_start = char_span.start\n",
    "            date_token_ids.append(token_start)\n",
    "            \n",
    "\n",
    "    pattern = [{\"POS\": \"NUM\", \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": date_pattern}}, {\"POS\": \"ADP\", \"OP\": \"?\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "    matcher.add(\"DATE_PATTERN\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    coords = {}\n",
    "    for match_id, start, end in matches:\n",
    "        if start in date_token_ids:\n",
    "            sent = doc[start:end].sent\n",
    "            if only_street_facts:\n",
    "                for street_name in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\"]:\n",
    "                    if street_name in sent.text:\n",
    "                        fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\" :[], \"entities_org\":[],\"description\": sent.text}\n",
    "                        for ent in sent.ents:\n",
    "                            if ent.label_ == \"LOC\":\n",
    "                                fact[\"entities_loc\"].append(ent.text)\n",
    "                            elif ent.label_ == \"PER\":\n",
    "                                fact[\"entities_per\"].append(ent.text)\n",
    "                            elif ent.label_ == \"ORG\":\n",
    "                                fact[\"entities_org\"].append(ent.text)\n",
    "\n",
    "                        if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                            if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                                coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                            fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                        else:\n",
    "                            fact[\"coordinates\"] = (None, None)\n",
    "                        historical_facts.append(fact)\n",
    "                    break\n",
    "            else:\n",
    "                fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\" :[], \"entities_org\":[],\"description\": sent.text}\n",
    "                for ent in sent.ents:\n",
    "                    if ent.label_ == \"LOC\":\n",
    "                        fact[\"entities_loc\"].append(ent.text)\n",
    "                    elif ent.label_ == \"PER\":\n",
    "                        fact[\"entities_per\"].append(ent.text)\n",
    "                    elif ent.label_ == \"ORG\":\n",
    "                        fact[\"entities_org\"].append(ent.text)\n",
    "\n",
    "                if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                    if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                        coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                    fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                else:\n",
    "                    fact[\"coordinates\"] = (None, None)\n",
    "                historical_facts.append(fact)\n",
    "    return historical_facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.lagarennecolombes.fr/Histoire%20et%20patrimoine/6042/6164\"\n",
    "def extract_historic_facts_regex(text):\n",
    "    # Expression régulière pour identifier les dates et les événements historiques\n",
    "    # Expression régulière pour identifier les dates\n",
    "    date_pattern = r\"(\\b(?:\\d{1,4}|X{1,3}I{1,3}|X{1,3}V?I{0,3}|V?I{1,3})\\s?(?:siècle|année|époque)\\b)\"\n",
    "\n",
    "    # Expression régulière pour identifier les lieux (rues, places, avenues, boulevards)\n",
    "    location_pattern = r\"(\\b(?:rue|place|avenue|boulevard)[^\\n,.!?]*\\b)\"\n",
    "\n",
    "    # Combinaison des expressions régulières\n",
    "    regex = re.compile(f\"({date_pattern}|{location_pattern})\", re.IGNORECASE)\n",
    "\n",
    "    # Découper le texte en phrases en utilisant un point, un point d'interrogation ou un point d'exclamation comme séparateur\n",
    "    sentences = re.split(r\"[.!?]\", text)\n",
    "    # Trouver les phrases qui correspondent à l'expression régulière\n",
    "    historic_facts = [sentence.strip() for sentence in sentences if regex.search(sentence)]\n",
    "\n",
    "    return historic_facts\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content_section = soup.find(\"div\", {\"class\": \"detail\"})\n",
    "    if content_section:\n",
    "        text = content_section.get_text()\n",
    "historic_facts = extract_historic_facts_regex(text)\n",
    "\n",
    "for fact in historic_facts:\n",
    "    print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cities = json.load(open(\"citiesv2.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "count = 0\n",
    "\n",
    "for city in cities:\n",
    "    if count < 10:\n",
    "        if city[\"name\"] != \"Marseille\":\n",
    "            query = f\"histoire de {city['name']}\"\n",
    "            print(query)\n",
    "            for j in search(query, num_results=1):\n",
    "                print(j)\n",
    "            \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_section_contents(url, regex_list):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    sections = \"\"\n",
    "\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex, re.IGNORECASE)\n",
    "\n",
    "        headers = soup.find_all(lambda tag: tag.name in {\"h2\"} and pattern.match(tag.text))\n",
    "\n",
    "        for header in headers:\n",
    "            content = \"\"\n",
    "            for sibling in header.find_next_siblings():\n",
    "                if sibling.name in {\"h2\"}:\n",
    "                    break\n",
    "                if sibling.name in {\"h3\", \"h4\", \"h5\", \"h6\"}:\n",
    "                    continue\n",
    "                sibling_text = re.sub(r'\\[\\d+\\]', '', sibling.text)\n",
    "                content += sibling_text.strip() + \"\\n\"\n",
    "\n",
    "            #section_title = header.text.strip().split('[')[0]\n",
    "            sections += content\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://fr.wikipedia.org/wiki/La_Garenne-Colombes\"\n",
    "regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "sections = get_section_contents(url, regex_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_facts = detect_historical_facts_v3(sections[list(sections.keys())[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates\n",
    "import requests\n",
    "def get_coordinates(address):\n",
    "    try:\n",
    "        params = {\n",
    "        \"q\": address,\n",
    "        \"format\": \"jsonv2\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(\"https://nominatim.openstreetmap.org/search\", params=params)\n",
    "        result = response.json()[0]\n",
    "\n",
    "        latitude = result[\"lat\"]\n",
    "        longitude = result[\"lon\"]\n",
    "    \n",
    "    except:\n",
    "        latitude = None\n",
    "        longitude = None\n",
    "    \n",
    "    return latitude, longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coordinates(\"place de la liberté, la garenne-colombes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal string\n",
    "from fuzzywuzzy import fuzz\n",
    "def equal_string(s1, s2, threshold=90):\n",
    "    return fuzz.token_set_ratio(s1, s2) > 90\n",
    "\n",
    "s1 = \"boulevard de verdun\"\n",
    "s2 = \"avenue de verdun\"\n",
    "equal_string(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_facts_wikipedia(url, city_name=\"\", only_streets=False):\n",
    "    regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "    histoire = get_section_contents(url, regex_list)\n",
    "    historic_facts = detect_historical_facts_v3(histoire, city_name, only_streets)\n",
    "    return historic_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "facts = {}\n",
    "last_len = 0\n",
    "for city in cities[:10]:\n",
    "    facts[city[\"id\"]] = find_facts_wikipedia(city[\"wikipediaUrl\"], city[\"name\"], True)\n",
    "    print(city[\"name\"], len(facts[city[\"id\"]]))\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for city in cities[-10:]:\n",
    "    count += len(facts[city[\"id\"]])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(facts, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
