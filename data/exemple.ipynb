{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def detect_historical_facts_v3(text, city_name=\"\", only_street_facts=False):\n",
    "\n",
    "    historical_facts = []\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    date_pattern = r\"\\b(?:\\d{1,2}\\s)?(?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre|\\d{1,2}(?:er)?\\s(?:siècle)|X{1,3}V{0,1}I{0,3}e\\s(?:siècle)|\\d{4})\\b\"\n",
    "    date_matches = [match for match in re.finditer(\n",
    "        date_pattern, text, re.IGNORECASE)]\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    date_token_ids = []\n",
    "\n",
    "    for match in date_matches:\n",
    "        date_start, date_end = match.span()\n",
    "        date_text = match.group()\n",
    "\n",
    "        char_span = doc.char_span(date_start, date_end)\n",
    "        if char_span is not None:\n",
    "            token_start = char_span.start\n",
    "            date_token_ids.append(token_start)\n",
    "\n",
    "    pattern = [{\"POS\": \"NUM\", \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": date_pattern}}, {\n",
    "        \"POS\": \"ADP\", \"OP\": \"?\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "    matcher.add(\"DATE_PATTERN\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    coords = {}\n",
    "    for match_id, start, end in matches:\n",
    "        if start in date_token_ids:\n",
    "            sent = doc[start:end].sent\n",
    "            if only_street_facts:\n",
    "                for street_name in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\", \"église\", \"île\", \"chapelle\"]:\n",
    "                    if street_name in sent.text.lower():\n",
    "                        fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\": [], \"entities_org\": [], \"description\": sent.text}\n",
    "                        for ent in sent.ents:\n",
    "                            if ent.label_ == \"LOC\":\n",
    "                                fact[\"entities_loc\"].append(ent.text)\n",
    "                            elif ent.label_ == \"PER\":\n",
    "                                fact[\"entities_per\"].append(ent.text)\n",
    "                            elif ent.label_ == \"ORG\":\n",
    "                                fact[\"entities_org\"].append(ent.text)\n",
    "                                '''\n",
    "                                if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                                    if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                                        coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                                    fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                                else:\n",
    "                                    fact[\"coordinates\"] = (None, None)\n",
    "                                '''\n",
    "\n",
    "                        historical_facts.append(fact)\n",
    "                        break\n",
    "            else:\n",
    "\n",
    "                fact = {\"date\": doc[start:end].text, \"entities_loc\": [], \"entities_per\": [], \"entities_org\": [], \"description\": sent.text}\n",
    "                for ent in sent.ents:\n",
    "                    if ent.label_ == \"LOC\":\n",
    "                        fact[\"entities_loc\"].append(ent.text)\n",
    "                    elif ent.label_ == \"PER\":\n",
    "                        fact[\"entities_per\"].append(ent.text)\n",
    "                    elif ent.label_ == \"ORG\":\n",
    "                        fact[\"entities_org\"].append(ent.text)\n",
    "                '''\n",
    "                if len(city_name) > 0 and len(fact[\"entities_loc\"]) > 0:\n",
    "                    if coords.get(max(fact['entities_loc'] , key=len) + \", \" + city_name) is None:\n",
    "                        coords[max(fact['entities_loc'] , key=len) + \", \" + city_name] = get_coordinates(max(fact['entities_loc'] , key=len) + \", \" + city_name)\n",
    "                    fact[\"coordinates\"] = coords[max(fact['entities_loc'] , key=len) + \", \" + city_name]\n",
    "                else:\n",
    "                    fact[\"coordinates\"] = (None, None)\n",
    "                '''\n",
    "                historical_facts.append(fact)\n",
    "    #historical_facts = [fact for fact in historical_facts if np.array([street in fact[\"description\"].lower() for street in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\",\"église\",\"île\",\"chapelle\"]]).any()]\n",
    "    return historical_facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.lagarennecolombes.fr/Histoire%20et%20patrimoine/6042/6164\"\n",
    "def extract_historic_facts_regex(text):\n",
    "    # Expression régulière pour identifier les dates et les événements historiques\n",
    "    # Expression régulière pour identifier les dates\n",
    "    date_pattern = r\"(\\b(?:\\d{1,4}|X{1,3}I{1,3}|X{1,3}V?I{0,3}|V?I{1,3})\\s?(?:siècle|année|époque)\\b)\"\n",
    "\n",
    "    # Expression régulière pour identifier les lieux (rues, places, avenues, boulevards)\n",
    "    location_pattern = r\"(\\b(?:rue|place|avenue|boulevard)[^\\n,.!?]*\\b)\"\n",
    "\n",
    "    # Combinaison des expressions régulières\n",
    "    regex = re.compile(f\"({date_pattern}|{location_pattern})\", re.IGNORECASE)\n",
    "\n",
    "    # Découper le texte en phrases en utilisant un point, un point d'interrogation ou un point d'exclamation comme séparateur\n",
    "    sentences = re.split(r\"[.!?]\", text)\n",
    "    # Trouver les phrases qui correspondent à l'expression régulière\n",
    "    historic_facts = [sentence.strip() for sentence in sentences if regex.search(sentence)]\n",
    "\n",
    "    return historic_facts\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content_section = soup.find(\"div\", {\"class\": \"detail\"})\n",
    "    if content_section:\n",
    "        text = content_section.get_text()\n",
    "historic_facts = extract_historic_facts_regex(text)\n",
    "\n",
    "for fact in historic_facts:\n",
    "    print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cities = json.load(open(\"citiesv3.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "count = 0\n",
    "\n",
    "for city in cities:\n",
    "    if count < 10:\n",
    "        if city[\"name\"] != \"Marseille\":\n",
    "            query = f\"histoire de {city['name']}\"\n",
    "            print(query)\n",
    "            for j in search(query, num_results=1):\n",
    "                print(j)\n",
    "            \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_section_contents(url, regex_list):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    sections = \"\"\n",
    "\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex, re.IGNORECASE)\n",
    "\n",
    "        headers = soup.find_all(lambda tag: tag.name in {\"h2\"} and pattern.match(tag.text))\n",
    "\n",
    "        for header in headers:\n",
    "            content = \"\"\n",
    "            for sibling in header.find_next_siblings():\n",
    "                if sibling.name in {\"h2\"}:\n",
    "                    break\n",
    "                if sibling.name in {\"h3\", \"h4\", \"h5\", \"h6\"}:\n",
    "                    continue\n",
    "                sibling_text = re.sub(r'\\[\\d+\\]', '', sibling.text)\n",
    "                content += sibling_text.strip() + \"\\n\"\n",
    "\n",
    "            #section_title = header.text.strip().split('[')[0]\n",
    "            sections += content\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://fr.wikipedia.org/wiki/Auvergne-Rh%C3%B4ne-Alpes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "count = 0\n",
    "facts = {}\n",
    "last_len = 0\n",
    "for city in cities[:10]:\n",
    "    sections = get_section_contents(city[\"wikipediaUrl\"], regex_list)\n",
    "    facts[city[\"id\"]] = detect_historical_facts_v3(sections, only_street_facts=False)\n",
    "    print(city[\"name\"], len(facts[city[\"id\"]]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates\n",
    "import requests\n",
    "def get_coordinates(address):\n",
    "    try:\n",
    "        params = {\n",
    "        \"q\": address,\n",
    "        \"format\": \"jsonv2\"\n",
    "        }\n",
    "        header = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(\"https://nominatim.openstreetmap.org/search\", params=params, headers=header)\n",
    "        result = response.json()[0]\n",
    "\n",
    "        latitude = result[\"lat\"]\n",
    "        longitude = result[\"lon\"]\n",
    "    \n",
    "    except:\n",
    "        latitude = None\n",
    "        longitude = None\n",
    "    \n",
    "    return latitude, longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coordinates(\"Saint-Denis Saint-Denis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal string\n",
    "from fuzzywuzzy import fuzz\n",
    "def equal_string(s1, s2, threshold=90):\n",
    "    return fuzz.token_set_ratio(s1, s2) > 90\n",
    "\n",
    "s1 = \"boulevard de verdun\"\n",
    "s2 = \"avenue de verdun\"\n",
    "equal_string(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_facts_wikipedia(url, city_name=\"\", only_streets=False):\n",
    "    regex_list = [r'(?i)\\bhist', r'(?i)\\bcultur', r'(?i)\\bpatrimo']\n",
    "    histoire = get_section_contents(url, regex_list)\n",
    "    historic_facts = detect_historical_facts_v3(histoire, only_street_facts=only_streets)\n",
    "    return historic_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "facts = {}\n",
    "last_len = 0\n",
    "for city in cities[:10]:\n",
    "    facts[city[\"id\"]] = find_facts_wikipedia(city[\"wikipediaUrl\"], city[\"name\"], False)\n",
    "    #facts[city[\"id\"]] = [fact for fact in facts[city[\"id\"]] if np.array([street in fact[\"description\"].lower() for street in [\"rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"voie\", \"cours\", \"impasse\", \"passage\", \"route\", \"square\", \"chemin\", \"rond-point\", \"pont\", \"cité\", \"esplanade\", \"promenade\", \"voie\", \"cathédrale\",\"église\",\"île\",\"chapelle\"]]).any()]\n",
    "    print(city[\"name\"], len(facts[city[\"id\"]]))\n",
    "    count += 1\n",
    "    if count >0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('street_facts_v2.json','r', encoding='utf-8') as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetNames = [\" rue\", \"avenue\", \"boulevard\", \"place\", \"quai\", \"allée\", \"impasse\", \"square\", \"chemin\", \"rond-point\", \"pont\",  \"esplanade\", \"promenade\", \"voie\", \"cathédrale\", \"église\", \"île\", \"chapelle\", \"collège\", \"lycée\",\"mairie\", \"école\", \"cimetière\", \"hôpital\", \"musée\", \"théâtre\", \"cinéma\", \"gare\", \"piscine\", \"parc\", \"jardin\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = {\n",
    "    \"rue\" : 0.0005,\n",
    "    \"avenue\" : 0.001,\n",
    "    \"boulevard\" : 0.002,\n",
    "    \"place\" : 0.0008,\n",
    "    \"allée\" : 0.0002,\n",
    "    \"impasse\" : 0.0001,\n",
    "    \"chemin\" : 0.0003,\n",
    "    \"cours\" : 0.0004,\n",
    "    \"quai\" : 0.0006,\n",
    "    \"passage\" : 0.0007,\n",
    "    \"square\" : 0.0009,\n",
    "    \"route\" : 0.0011,\n",
    "    \"rond-point\" : 0.001,\n",
    "    \"voie\" : 0.0005,\n",
    "    \"promenade\" : 0.0002,\n",
    "    \"parc\" : 0.001,\n",
    "    \"cité\" : 0.0008,\n",
    "    \"esplanade\" : 0.001,\n",
    "    \"cathédrale\" : 0.0001,\n",
    "    \"église\" : 0.0001,\n",
    "    \"île\" : 0.001,\n",
    "    \"chapelle\" : 0.0001,\n",
    "    \"collège\" : 0.00001,\n",
    "    \"lycée\" : 0.00001,\n",
    "    \"mairie\" : 0.00001,\n",
    "    \"école\" : 0.00001,\n",
    "    \"cimetière\" : 0.00001,\n",
    "    \"hôpital\" : 0.00001,\n",
    "    \"musée\" : 0.00001,\n",
    "    \"théâtre\" : 0.00001,\n",
    "    \"cinéma\" : 0.00001,\n",
    "    \"gare\" : 0.00001,\n",
    "    \"piscine\" : 0.00001,\n",
    "    \"jardin\" : 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('street_facts_v3.json','w', encoding='utf-8') as f:\n",
    "    json.dump(facts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"citiesv3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "known_places = {}\n",
    "street_fact = {}\n",
    "i = 0\n",
    "for city in facts.keys():\n",
    "    street_fact[city] = []\n",
    "    for fact in facts[city]:\n",
    "        for street in streetNames:\n",
    "            if street+\" \" in fact[\"description\"].lower() or street+\".\" in fact[\"description\"].lower():\n",
    "                fact[\"area\"] = size[street] if street in size.keys() else None\n",
    "                if len(fact[\"entities_loc\"]) != 0:\n",
    "                    key_place = max(fact[\"entities_loc\"])+ \" \" + cities[city][\"name\"]\n",
    "                    if key_place not in known_places.keys():\n",
    "                        known_places[key_place] = get_coordinates(key_place)\n",
    "                    fact[\"coord\"] = known_places[key_place]\n",
    "                    fact[\"street_loc\"] = True\n",
    "                    if fact[\"coord\"] == (None, None):\n",
    "                        fact[\"coord\"] = cities[city][\"coordinates\"]\n",
    "                        fact[\"street_loc\"] = False\n",
    "                else:\n",
    "                    fact[\"coord\"] = cities[city][\"coordinates\"]\n",
    "                    fact[\"street_loc\"] = False\n",
    "                print(fact[\"coord\"], key_place)\n",
    "                street_fact[city].append(fact)\n",
    "                break\n",
    "    i += 1\n",
    "    print(i, end=\"-\")\n",
    "    if i > 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('street_facts_v3.json','w', encoding='utf-8') as f:\n",
    "    json.dump(facts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(\"./facts_dep_reg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./regions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    regions = json.load(f)\n",
    "with open(\"./departements.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    departements = json.load(f)\n",
    "\n",
    "dep_regs = {**regions, **departements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_places = {}\n",
    "street_fact = {}\n",
    "non_street_fact = {}\n",
    "i = 0\n",
    "for city in facts.keys():\n",
    "    street_fact[city] = []\n",
    "    non_street_fact[city] = []\n",
    "    for fact in facts[city]:\n",
    "        found_street = False\n",
    "        for street in streetNames:\n",
    "            if any(street+ delim in fact[\"description\"].lower() for delim in [\" \",\".\"]):\n",
    "                if(len(fact[\"entities_loc\"]) != 0):\n",
    "                    key_place = max(fact[\"entities_loc\"])+ \" \" + dep_regs[city][\"name\"]\n",
    "                    if key_place not in known_places.keys():\n",
    "                        known_places[key_place] = get_coordinates(key_place)\n",
    "                    fact[\"coordinates\"] = known_places[key_place]\n",
    "                    fact[\"street_loc\"] = True\n",
    "                    if fact[\"coordinates\"] == (None, None):\n",
    "                        fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "                        fact[\"street_loc\"] = False\n",
    "                else:\n",
    "                    fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "                    fact[\"street_loc\"] = False\n",
    "                fact[\"area\"] = size[street] if street in size.keys() else None\n",
    "                street_fact[city].append(fact)\n",
    "                found_street = True\n",
    "                break\n",
    "        if not found_street:\n",
    "            fact[\"coordinates\"] = dep_regs[city][\"coordinates\"]\n",
    "            fact[\"area\"] = dep_regs[city][\"area\"]\n",
    "            non_street_fact[city].append(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for streetfact in street_fact:\n",
    "    count += len(street_fact[streetfact])\n",
    "print(count)\n",
    "\n",
    "for nonstreetfact in non_street_fact:\n",
    "    count += len(non_street_fact[nonstreetfact])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for factnez in facts:\n",
    "    count += len(facts[factnez])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"street_facts_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(street_fact, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_street_facts_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(non_street_fact, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "facts = json.load(open(\"./facts_dep_reg.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_duplicates = {}\n",
    "count = 0\n",
    "for city in facts.keys():\n",
    "    no_duplicates[city] = []\n",
    "    last_fact = \"\"\n",
    "    for fact in facts[city]:\n",
    "        if fact[\"description\"] in last_fact:            \n",
    "            no_duplicates[city][-1][\"date\"] += \"\" if any([date in no_duplicates[city][-1][\"date\"] for date in fact[\"date\"].split(\" \")]) else \" \" + fact[\"date\"]\n",
    "            no_duplicates[city][-1][\"entities_loc\"] += [fact_ for fact_ in fact[\"entities_loc\"] if fact_ not in no_duplicates[city][-1][\"entities_loc\"]]\n",
    "            no_duplicates[city][-1][\"entities_per\"] += [fact_ for fact_ in fact[\"entities_per\"] if fact_ not in no_duplicates[city][-1][\"entities_per\"]]\n",
    "            no_duplicates[city][-1][\"entities_org\"] += [fact_ for fact_ in fact[\"entities_org\"] if fact_ not in no_duplicates[city][-1][\"entities_org\"]]\n",
    "        else:\n",
    "            no_duplicates[city].append(fact)\n",
    "            count += 1\n",
    "        last_fact = fact[\"description\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./street_facts_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./regions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    regions = json.load(f)\n",
    "with open(\"./departements.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    departements = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "clean_facts = {}\n",
    "vague_terms = [\"cela\", \"ceci\", \"ça\",\"il\",\"elle\",\"ils\",\"elles\",\"on\",\"se\",\"sa\",\"son\",\"ses\",\"leur\",\"leurs\"]\n",
    "for city, facts_list in facts.items():\n",
    "    clean_facts[city] = []\n",
    "    for fact in facts_list:\n",
    "        add_fact = True\n",
    "        fact[\"description\"] = fact[\"description\"].strip(\" \\n\")\n",
    "        if fact[\"description\"].split(\" \")[0].lower() in vague_terms or fact[\"description\"][-1] not in [\".\",\"!\",\"?\",\";\"] or fact[\"description\"][0].islower():\n",
    "            add_fact = False\n",
    "        if add_fact:       \n",
    "            clean_facts[city].append(fact)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for city, facts_list in clean_facts.items():\n",
    "    count += len(facts_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(DATABASE_URL)\n",
    "\n",
    "# Remplacez 'your_database_name' par le nom de votre base de données\n",
    "db = client.test\n",
    "\n",
    "# Remplacez 'your_collection_name' par le nom de la collection que vous souhaitez utiliser\n",
    "facts_collection = db.facts\n",
    "\n",
    "# Inserez many\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities facts\n",
    "import json\n",
    "with open(\"facts_dep_reg_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts_dep_reg = json.load(f)\n",
    "with open(\"facts_city_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts_cities = json.load(f)\n",
    "\n",
    "facts = {**facts_dep_reg, **facts_cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"citiesv3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cities = json.load(f)\n",
    "with open(\"./regions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    regions = json.load(f)\n",
    "with open (\"./departements.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    departements = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_json = {**regions, **departements, **cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_person_collection = db.historicalPeople\n",
    "location_collection = db.locations\n",
    "fact_history_collection = db.factHistoricalPeople\n",
    "fact_location_collection = db.factLocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_db = []\n",
    "for city in facts.keys():\n",
    "\n",
    "    for fact in facts[city]:\n",
    "        fact_db = {}\n",
    "        fact_db[\"title\"] = \"\"\n",
    "        fact_db[\"content\"] = fact[\"description\"]\n",
    "        fact_db[\"from\"] = fact[\"date\"]\n",
    "        fact_db[\"until\"] = fact[\"date\"]\n",
    "        fact_db[\"locationId\"] = locations_db[city][\"_id\"]\n",
    "        fact_db[\"personsInvolved\"] = []\n",
    "        facts_db.append(fact_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "historical_ppl = {}\n",
    "for loc in facts.keys():\n",
    "    for fact in facts[loc]:\n",
    "        for person in fact[\"entities_per\"]:\n",
    "            if len(person)>=5:\n",
    "                if person not in historical_ppl.keys():\n",
    "                    if \"\\n\" not in person:\n",
    "                        historical_ppl[person] = {\"name\": person, \"FactHistoricalPerson\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserer les locations dans la base de données\n",
    "results = facts_collection.insert_many(list(facts_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facthistoperson_db = []\n",
    "i = 0\n",
    "for loc in facts:\n",
    "    for fact in facts[loc]:\n",
    "        for person in fact[\"entities_per\"]:\n",
    "            if person in  historical_ppl.keys():\n",
    "                facthistoperson_db.append({\"factId\": facts_db[i][\"_id\"] ,\"historicalPersonId\": historical_ppl[person][\"_id\"]})\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for facthisto in facthistoperson_db:\n",
    "    pers  = historical_person_collection.find_one({\"_id\": facthisto[\"historicalPersonId\"]})\n",
    "    fac = facts_collection.find_one({\"_id\": facthisto[\"factId\"]})\n",
    "    print(pers[\"name\"] + \"//\", fac[\"content\"][:50])\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facthistoperson_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_history_collection.insert_many(list(facthistoperson_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = location_collection.find({\"type\":\"region\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import json_util\n",
    "\n",
    "# save locations in json file\n",
    "with open(\"historical_people.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json.loads(json_util.dumps(historical_ppl)), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in facts:\n",
    "    for fact in facts[loc]:\n",
    "        for person in fact[\"entities_per\"]:\n",
    "            if len(person)>=5:\n",
    "                print(person + \"//\", fact[\"description\"][:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all facts in the database if the content dont start with a letter \n",
    "res = facts_collection.delete_many({\"content\": {\"$regex\":r\"[^()]*\\([^()]*$|^[^()]*\\)|^[^()]*\\([^()]*[^()]*$\"}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.deleted_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of element in fact collection\n",
    "res = facts_collection.find({})\n",
    "count = 0\n",
    "for r in res:\n",
    "    count += 1\n",
    "print(count)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commence par maj finit par point\n",
    "commence par une lettre\n",
    "pas de \\n dedans,\n",
    "commence par une lettres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_list = []\n",
    "for loc in facts.keys():\n",
    "    for fact in facts[loc]:\n",
    "        facts_list.append(fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"^(Mais|Cependant|Toutefois|Alors|Où|Et|Donc|Or|Ni|Car|Parce que|Ainsi|En outre|De plus|Par ailleurs|En revanche|Néanmoins|Pourtant|De même|En effet|Par conséquent|Par exemple|C'est-à-dire|D'autre part|En somme|Quant à|A contrario|De surcroît|D'ailleurs|Puisque|Bien que|[Cc]e|[Cc]ette|[Cc]et|[Ii]l|[Ee]lle|[Ii]ls|[Ee]lles|[Cc]ela|[Cc]eci|[Çç]a|[Oo]n|[Ss]e|[Ss]a|[Ss]on|[Ss]es|[Ll]eur|[Ll]eurs) \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "count = 0\n",
    "for fact in facts_list:\n",
    "    if not fact[\"description\"][0].isalpha() or fact[\"description\"].count('\\n') > 0 or re.search(pattern, fact[\"description\"]) is not None:\n",
    "        print(fact[\"description\"])\n",
    "        print(\"--\"*20)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Client hasn't been generated yet, you must run `prisma generate` before you can use the client.\nSee https://prisma-client-py.readthedocs.io/en/stable/reference/troubleshooting/#client-has-not-been-generated-yet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprisma\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[0;32m      4\u001b[0m prisma \u001b[39m=\u001b[39m Client()\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sabat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prisma\\__init__.py:43\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39m# TODO: support checking for 'models' here too\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mPrisma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mClient\u001b[39m\u001b[39m'\u001b[39m}:\n\u001b[0;32m     42\u001b[0m         \u001b[39m# TODO: remove this frame from the stack trace\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe Client hasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt been generated yet, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m             \u001b[39m'\u001b[39m\u001b[39myou must run `prisma generate` before you can use the client.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     46\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mSee https://prisma-client-py.readthedocs.io/en/stable/reference/troubleshooting/#client-has-not-been-generated-yet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     47\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     49\u001b[0m     \u001b[39m# leaves handling of this potential error to Python as per PEP 562\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The Client hasn't been generated yet, you must run `prisma generate` before you can use the client.\nSee https://prisma-client-py.readthedocs.io/en/stable/reference/troubleshooting/#client-has-not-been-generated-yet"
     ]
    }
   ],
   "source": [
    "from prisma import Client\n",
    "from datetime import datetime\n",
    "\n",
    "prisma = Client()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
